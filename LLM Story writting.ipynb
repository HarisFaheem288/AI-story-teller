{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "216df6f0-2ebc-482a-8e7b-0c2d31b62f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data4.txt...\n",
      "Processing data1.csv...\n",
      "Processing data2.csv...\n",
      "Processing data3.csv...\n",
      "Processing data5.csv...\n",
      "Saving 38427 chunks to preprocessed_stories.jsonl...\n",
      "Preprocessing done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Basic cleaning: remove multiple spaces, newlines, special chars\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, max_words=100):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk = ' '.join(words[i:i+max_words])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def preprocess_txt(file_path, source_name):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    # Split by paragraphs (empty line)\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    chunks = []\n",
    "    for para in paragraphs:\n",
    "        para = clean_text(para)\n",
    "        chunks.extend(chunk_text(para))\n",
    "    data = [{'id': f'{source_name}_txt_{i}', 'text': chunk, 'source': source_name} for i, chunk in enumerate(chunks)]\n",
    "    return data\n",
    "\n",
    "def preprocess_csv(file_path, text_column, source_name):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        idx = 0\n",
    "        for row in reader:\n",
    "            if text_column not in row or not row[text_column]:\n",
    "                continue\n",
    "            text = clean_text(row[text_column])\n",
    "            chunks = chunk_text(text)\n",
    "            for chunk in chunks:\n",
    "                data.append({'id': f'{source_name}_csv_{idx}', 'text': chunk, 'source': source_name})\n",
    "                idx += 1\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "    # Paths to your datasets\n",
    "    txt_file = 'data4.txt'  # replace with your txt filename\n",
    "    csv_files = [\n",
    "        ('data1.csv', 'story_text'),  # replace with your csv filename & text column\n",
    "        ('data2.csv', 'text_column_name'),\n",
    "        ('data3.csv', 'text_column_name'),\n",
    "        ('data5.csv', 'text_column_name'),\n",
    "    ]\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # Preprocess txt file\n",
    "    print(f'Processing {txt_file}...')\n",
    "    all_data.extend(preprocess_txt(txt_file, 'dataset1'))\n",
    "\n",
    "    # Preprocess csv files\n",
    "    for file_path, text_col in csv_files:\n",
    "        print(f'Processing {file_path}...')\n",
    "        all_data.extend(preprocess_csv(file_path, text_col, os.path.splitext(file_path)[0]))\n",
    "\n",
    "    # Save combined data to jsonl\n",
    "    output_file = 'preprocessed_stories.jsonl'\n",
    "    print(f'Saving {len(all_data)} chunks to {output_file}...')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in all_data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "    print('Preprocessing done.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1bfba1-b5b3-499d-859a-f820d0e536c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442c217083ac47ca983dbb51372f735e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/601 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding embeddings to FAISS index...\n",
      "Indexing complete.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# Load your preprocessed data\n",
    "data_file = 'preprocessed_stories.jsonl'\n",
    "texts = []\n",
    "ids = []\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with open(data_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        texts.append(item['text'])\n",
    "        ids.append(item['id'])\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = model.encode(texts, batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "print(\"Adding embeddings to FAISS index...\")\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save FAISS index and metadata\n",
    "faiss.write_index(index, 'stories_index.faiss')\n",
    "\n",
    "with open('stories_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump({'ids': ids, 'texts': texts}, f)\n",
    "\n",
    "print(\"Indexing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436f1741-7ac7-4eab-9f23-c0801731931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: their own front dooryard. Decidedly, the inhabitant, if there were one, must be of kin to the wildwood creatures, for his dwelling and its surroundings evidently belonged as much to the forest people as to him. On the day when my story begins, the house in the wood was the only lifeless thing, or so it seemed, in the whole joyous little scene. It was a day in early May, and the world was so delighted with itself that it laughed and twinkled all over. The trees were hardly yet in full leaf, but had the gray-green misty look of\n",
      "\n",
      "Chunk 2: just a little way beyond, and so as he walked straight ahead he was getting farther and farther away from the river. It was very hard to walk in the jungle. The sticky leaves of the ferns caught at my father's hair, and he kept tripping over roots and rotten logs. Sometimes the trees were clumped so closely together that he couldn't squeeze between them and had to walk a long way around. He began to hear whispery noises, but he couldn't see any animals anywhere. The deeper into the jungle he went the surer he was that something was\n",
      "\n",
      "Chunk 3: forest, â€˜sclusively full of trees and bushes and stripy, speckly, patchy-blatchy shadows, and there they hid: and after another long time, what with standing half in the shade and half out of it, and what with the slippery-slidy shadows of the trees falling on them, the Giraffe grew blotchy, and the Zebra grew stripy, and the Eland and the Koodoo grew darker, with little wavy grey lines on their backs like bark on a tree trunk; and so, though you could hear them and smell them, you could very seldom see them, and then only when you knew precisely where\n",
      "\n",
      "Chunk 4: through the underbrush they entered another forest, where the trees were bigger and older than any they had ever seen. \"This forest is perfectly delightful,\" declared the Lion, looking around him with joy. \"Never have I seen a more beautiful place.\" \"It seems gloomy,\" said the Scarecrow. \"Not a bit of it,\" answered the Lion. \"I should like to live here all my life. See how soft the dried leaves are under your feet and how rich and green the moss is that clings to these old trees. Surely no wild beast could wish a pleasanter home.\" \"Perhaps there are\n",
      "\n",
      "Chunk 5: It is possible that there are other forests in the world, but they cannot be so fine as this, so we call ours 'the forest.'\" \"Are there pleasant neighbors here?\" asked Mr. Red Squirrel. \"Very good, very good. My wife and I do not call on many of them, but still they are good enough people, I think.\" \"Then why don't you call?\" \"Why? Why? Because they are not in our set. It would never do.\" And the Gray Squirrel sat up very straight indeed. \"Who is that gliding fellow on the ground below?\" asked the newcomer. \"Is he one\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load FAISS index and metadata\n",
    "index = faiss.read_index('stories_index.faiss')\n",
    "with open('stories_metadata.json', 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def retrieve_relevant_chunks(query, top_k=5):\n",
    "    query_vec = model.encode([query])\n",
    "    distances, indices = index.search(query_vec, top_k)\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        chunk_text = metadata['texts'][idx]\n",
    "        chunk_id = metadata['ids'][idx]\n",
    "        results.append({'id': chunk_id, 'text': chunk_text})\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "prompt = \"A magical forest adventure with mysterious creatures\"\n",
    "retrieved = retrieve_relevant_chunks(prompt, top_k=5)\n",
    "for i, chunk in enumerate(retrieved, 1):\n",
    "    print(f\"Chunk {i}: {chunk['text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac556d03-ca7c-4953-bb92-f9e13965a194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Git\\New folder\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1001: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "D:\\Git\\New folder\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0f0b424b4c4582be58fced1e9fb803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving locally...\n",
      "âœ… Model saved to ./local_mistral_model1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "token = \"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\"  # your HF token\n",
    "\n",
    "print(\"Downloading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", use_auth_token=token)\n",
    "\n",
    "print(\"Saving locally...\")\n",
    "model.save_pretrained(\"./local_mistral_model1\")\n",
    "tokenizer.save_pretrained(\"./local_mistral_model1\")\n",
    "print(\"âœ… Model saved to ./local_mistral_model1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec447e9a-f373-41a1-bf03-7b9a1d6f9531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83716f9dedc2463ca2985eefbcd53064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a magical forest adventure with mysterious creatures. Your protagonist, an aspiring wizard named Eliot, stumbles upon an ancient talking oak that reveals a hidden realm within the enchanted woods.\n",
      "\n",
      "\n",
      "**Solution 1:**\n",
      "\n",
      "Eliot had always felt a twinge of magic in his bones, a longing for adventure beyond the mundane streets of his village. That is until one starlit night, while practicing his spellcasting, a gentle voice tickled his ears.\n",
      "\n",
      "\n",
      "\"Tread lightly, Eliot, for you tread on hallowed ground,\" whispered the ancient talking oak. Its leaves shimmered with an ethereal glow.\n",
      "\n",
      "\n",
      "Startled but intrigued, Eliot knelt beside the great tree, bowing slightly. \"Who's there?\" he asked.\n",
      "\n",
      "\n",
      "\"I am Aelwyn, guardian of the Whispering Woods. Secrets are buried beneath my roots, and a hidden realm awaits the worthy. If you seek adventure, come.\"\n",
      "\n",
      "\n",
      "His heart swelled with wonder. Eliot's fingers brushed against the bark, and the forest's magic surged through him, opening a hidden gate to a world where the moon kissed the treetops and the stars hung low and low.\n",
      "\n",
      "\n",
      "The wizard stepped through, his eyes widening at\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Path to your saved model\n",
    "model_path = \"./local_mistral_model1\"\n",
    "\n",
    "# Select device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer & model from local folder\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Function to generate text\n",
    "def generate_story(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=300, do_sample=True, temperature=0.8)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Try it\n",
    "prompt = \"Write a magical forest adventure with mysterious creatures.\"\n",
    "print(generate_story(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b883f7f1-5310-4b18-acaf-b0d9b2c5978e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aa4efd79fb4e5496766b3f99346525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following story fragments as inspiration:\n",
      "\"Come, little birthday present,\" he said tenderly. \"The dragon will be so pleased. And I'm glad to see you're not crying. You know, my child, we cannot begin too young to learn to think of the happiness of others rather than our own. I should not like my dear little niece to be selfish, or to wish to deny a trivial pleasure to a poor, sick dragon, far from his home and friends.\" The Princess said she would try not to be selfish. Presently the cab drew up near the pillar, and there was the dragon, his ugly purple head\n",
      "The Book of DRAGONS\n",
      "Jason, turn back before it is too late. It would grieve us to the heart, if you and your nine and forty brave companions should be eaten up, at fifty mouthfuls, by this execrable dragon.\" \"My young friends,\" quietly replied Jason, \"I do not wonder that you think the dragon very terrible. You have grown up from infancy in the fear of this monster, and therefore still regard him with the awe that children feel for the bugbears and hobgoblins which their nurses have talked to them about. But, in my view of the matter, the dragon is merely a\n",
      "\n",
      "Now write a new story based on: A brave young girl who befriends a dragon.\n",
      "\n",
      "Once upon a time, in a land that was a mishmash of reality and legend, there lived a young girl named Elara. Elara was unlike any other child in her village; she was fearless and possessed a heart brimming with compassion. The villagers often whispered about her encounters with the creatures of the wood, but none spoke of the dragon.\n",
      "\n",
      "Elara had heard tales of a dragon, an ancient being with scales that shimmered like the night sky, and eyes that held the wisdom of centuries. Most of the villagers regarded the dragon as a menace, but Elara felt a connection to it. She believed that every creature, regardless of its appearance, held a story worth listening to.\n",
      "\n",
      "One day, as Elara ventured deeper into the forest than she had ever dared before, she heard a low, melancholic whimper. Following the sound, she came upon a dragon that seemed to be mourning. The dragon's scales were dulled by sorrow, and its wings drooped as it sheltered beneath a great oak.\n",
      "\n",
      "Tentatively, Elara approached the dragon. \"Why so sorrowful, mighty creature?\" she asked, her voice steady despite the nerves that fluttered in her stomach.\n",
      "\n",
      "The dragon lifted its head, and in a voice as deep and resonant as thunder, it replied, \"I am weary of living alone. The forest that I once called home has been overrun by greed and destruction. My kind has been chased away, and my heart has been torn by the loneliness.\"\n",
      "\n",
      "Elara listened with empathy, her heart swelling with a desire to help. \"I will not let you suffer in solitude,\" she promised. \"I will stand beside you.\"\n",
      "\n",
      "And so, El\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ===== Load FAISS index =====\n",
    "index = faiss.read_index('stories_index.faiss')\n",
    "with open('stories_metadata.json', 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# ===== Load embedding model =====\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# ===== Load Mistral model =====\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"./local_mistral_model1\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ===== Retrieval Function =====\n",
    "def retrieve_chunks(query, top_k=3):\n",
    "    query_vec = embed_model.encode([query])\n",
    "    distances, indices = index.search(query_vec, top_k)\n",
    "    results = [metadata['texts'][i] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "# ===== Story Generation Function =====\n",
    "def generate_story_with_context(query):\n",
    "    context_chunks = retrieve_chunks(query)\n",
    "    context_text = \"\\n\".join(context_chunks)\n",
    "\n",
    "    final_prompt = f\"Use the following story fragments as inspiration:\\n{context_text}\\n\\nNow write a new story based on: {query}\"\n",
    "\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400, do_sample=True, temperature=0.8)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ===== Test =====\n",
    "user_prompt = \"A brave young girl who befriends a dragon\"\n",
    "print(generate_story_with_context(user_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f6429a8-86a6-44bf-8c84-250f8c996561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb04e0b9b24947bfbe3326a56f8914a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4a52bdaacf4913b3c79229cec60ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Load the Stable Diffusion model\n",
    "model_id = \"./local_mistral_model2\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# Move the pipeline to the chosen device (GPU or CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "# Generate an image\n",
    "prompt = \"A brave young girl who befriends a dragon in a magical forest\"\n",
    "image = pipe(prompt).images[0]\n",
    "\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1824f74-5bf0-44e3-b3a7-3a54fa4f75a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "âœ… Audio generated and saved as story_audio.wav\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "# Initialize TTS engine\n",
    "engine = pyttsx3.init()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set voice properties (optional)\n",
    "engine.setProperty('rate', 160)   # Speed of speech\n",
    "engine.setProperty('volume', 1.0)  # Volume (0.0 to 1.0)\n",
    "\n",
    "# Example generated text (replace with your Mistral output)\n",
    "generated_text = \"Once upon a time in a magical forest, a tiny fox found a glowing blue crystal.\"\n",
    "\n",
    "# Save to file\n",
    "engine.save_to_file(generated_text, \"story_audio.wav\")\n",
    "\n",
    "# Run the engine to process the speech\n",
    "engine.runAndWait()\n",
    "\n",
    "print(\"âœ… Audio generated and saved as story_audio.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de72fc61-b75f-4371-9a3d-c78d2bec81b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c8d9f9ad104516832debbc66c1cd17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e1f61c6cb94cd993e95c3da29fe7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“– Generating story...\n",
      "\n",
      "ðŸŽ¨ Generating image...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c668c74a774dd39f9001c0f2d13130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Š Generating audio...\n",
      "\n",
      "âœ… Done!\n",
      "Story saved as audio: story_audio.wav\n",
      "Image saved as: generated_image.png\n",
      "\n",
      "Generated Story:\n",
      "\n",
      "\"Come, little birthday present,\" he said tenderly. \"The dragon will be so pleased. And I'm glad to see you're not crying. You know, my child, we cannot begin too young to learn to think of the happiness of others rather than our own. I should not like my dear little niece to be selfish, or to wish to deny a trivial pleasure to a poor, sick dragon, far from his home and friends.\" The Princess said she would try not to be selfish. Presently the cab drew up near the pillar, and there was the dragon, his ugly purple head\n",
      "for a minute, but she saw it quite plainly, and she said to herself: \"Dear me, what a curious, shiny, bright-looking creature! If it were bigger, and if I didn't know that there have been no fabulous monsters for quite a long time now, I should almost think it was a dragon.\" The thing, whatever it was, did look rather like a dragon but then it was too small; and it looked rather like a lizard only then it was too big. It was about as long as a hearthrug. \"I wish it had not been in such a hurry\n",
      "The Book of DRAGONS\n",
      "\n",
      "Now write a creative, original story based on: A brave young girl who befriends a dragon in a magical forest\n",
      "Only output the story itself, without mentioning the background.\n",
      "\n",
      "**Solution:**\n",
      "\n",
      "In the heart of the Enchanted Forest of Eldoria, where the trees whispered secrets and the rivers sang melodies, there lived a young girl named Aria. With a laugh as bright as the morning sun and eyes filled with the wonder of countless stars, Aria was the forest's little guardian spirit.\n",
      "\n",
      "One day, while wandering through the forest's shadowy thicket, she stumbled upon an unusual creature. It was unlike anything she had ever seen; iridescent scales shimmered in the dappled sunlight, its eyes held a wisdom far beyond its years. Aria stood frozen, her heart pounding with a mix of fear and curiosity.\n",
      "\n",
      "\"Hello,\" she whispered, her voice barely audible over the rustle of leaves.\n",
      "\n",
      "The creature opened its mouth, and a soothing breeze filled the air. \"Greetings, little one. I know not your name, but I sense the kindness in your soul.\"\n",
      "\n",
      "Aria stepped closer, her fear giving way to fascination. \"You're a dragon?\" she asked, disbelief coloring her words.\n",
      "\n",
      "\"Only in the way that some are born, and some are chosen,\" the creature replied with a gentle rumble. \"And you, dear child, have chosen to visit me.\"\n",
      "\n",
      "Aria took a tentative step forward. \"I... I've always dreamed of meeting a dragon. I've read about them in the stories told by the villagers. But they say dragons are fierce, and we should fear them.\"\n",
      "\n",
      "The creature's eyes grew warm, and it spoke with a voice that seemed to echo through the ages. \"Fear not, brave Aria. I am no threat, but a friend. The stories are but shadows of\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import pyttsx3\n",
    "import torch\n",
    "\n",
    "# ======== SETTINGS ========\n",
    "story_model_path = \"./local_mistral_model1\"   # Your Phi-3 model path\n",
    "image_model_id = \"./local_mistral_model2\"   # Lightweight image generation model\n",
    "faiss_index_path = \"stories_index.faiss\"\n",
    "faiss_metadata_path = \"stories_metadata.json\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ======== 1. Load FAISS Index & Embedding Model ========\n",
    "index = faiss.read_index(faiss_index_path)\n",
    "with open(faiss_metadata_path, 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# ======== 2. Load Phi-3 Model for Story Generation ========\n",
    "tokenizer = AutoTokenizer.from_pretrained(story_model_path)\n",
    "story_model = AutoModelForCausalLM.from_pretrained(\n",
    "    story_model_path,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# ======== 3. Load Image Generation Model (stabilityai/sd-turbo) ========\n",
    "image_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    image_model_id,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "# ======== Retrieval Function ========\n",
    "def retrieve_chunks(query, top_k=3):\n",
    "    query_vec = embed_model.encode([query])\n",
    "    distances, indices = index.search(query_vec, top_k)\n",
    "    return [metadata['texts'][i] for i in indices[0]]\n",
    "\n",
    "# ======== Story Generation Function ========\n",
    "def generate_story_with_context(query):\n",
    "    # Retrieve relevant context\n",
    "    context_chunks = retrieve_chunks(query)\n",
    "    context_text = \"\\n\".join(context_chunks)\n",
    "\n",
    "    # Hidden context: model sees it, user doesn't\n",
    "    final_prompt = (\n",
    "        f\"The following text contains background information for writing a story. \"\n",
    "        f\"Do NOT copy or list this background directly; just use it to inspire the writing.\\n\\n\"\n",
    "        f\"Background:\\n{context_text}\\n\\n\"\n",
    "        f\"Now write a creative, original story based on: {query}\\n\"\n",
    "        f\"Only output the story itself, without mentioning the background.\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = story_model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=400, \n",
    "        do_sample=True, \n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "    story = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Optional: strip any leftover prompt text (in case the model repeats it)\n",
    "    if \"Background:\" in story:\n",
    "        story = story.split(\"Background:\")[-1].strip()\n",
    "\n",
    "    return story\n",
    "# ======== Image Generation Function ========\n",
    "def generate_image(prompt):\n",
    "    image = image_pipe(prompt).images[0]\n",
    "    image_path = \"generated_image.png\"\n",
    "    image.save(image_path)\n",
    "    return image_path\n",
    "\n",
    "# ======== Text-to-Speech Function ========\n",
    "def text_to_speech(text):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty('rate', 160)\n",
    "    engine.setProperty('volume', 1.0)\n",
    "    audio_path = \"story_audio.wav\"\n",
    "    engine.save_to_file(text, audio_path)\n",
    "    engine.runAndWait()\n",
    "    return audio_path\n",
    "\n",
    "# ======== Main Pipeline ========\n",
    "def run_pipeline(user_prompt):\n",
    "    print(\"\\nðŸ“– Generating story...\")\n",
    "    story = generate_story_with_context(user_prompt)\n",
    "\n",
    "    print(\"\\nðŸŽ¨ Generating image...\")\n",
    "    img_path = generate_image(user_prompt)\n",
    "\n",
    "    print(\"\\nðŸ”Š Generating audio...\")\n",
    "    audio_path = text_to_speech(story)\n",
    "\n",
    "    print(\"\\nâœ… Done!\")\n",
    "    print(f\"Story saved as audio: {audio_path}\")\n",
    "    print(f\"Image saved as: {img_path}\")\n",
    "    print(\"\\nGenerated Story:\\n\")\n",
    "    print(story)\n",
    "\n",
    "# ======== Run ========\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"A brave young girl who befriends a dragon in a magical forest\"\n",
    "    run_pipeline(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61ac07-3c0d-4ebb-88bf-7010365736f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
